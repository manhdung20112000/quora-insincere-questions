{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["import os\n","import re\n","import nltk\n","import pandas as pd \n","import numpy as np\n","from tqdm import tqdm\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","import torch\n","import torchtext\n","from torchtext import data\n","from torch import nn\n","from torch.nn import functional as F\n","from torch import optim"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:36.312502Z","iopub.execute_input":"2021-06-10T22:54:36.312872Z","iopub.status.idle":"2021-06-10T22:54:38.794670Z","shell.execute_reply.started":"2021-06-10T22:54:36.312796Z","shell.execute_reply":"2021-06-10T22:54:38.793719Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# I. Load dataset and pre-processing\n","\n","**Text preprocessing** is traditionally an important step for natural language processing (NLP) tasks. \n","\n","In the pre-processing task, I did:\n","- Remove contractions\n","- Remove punctuation\n","- Tokenization\n","\n","I didn't remove `stopwords` because of deep learning model that were used later can be affect. I predicted that it counld be harder to learn the context if we removed `stopwords`."],"metadata":{}},{"cell_type":"markdown","source":["## Load dataset"],"metadata":{}},{"cell_type":"code","source":["DATA_PATH = '../input/quora-insincere-questions-classification/'\n","\n","train_df = pd.read_csv('../input/train-dataset-quora-insincere-questions/out.csv')\n","test_df  = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:54:58.996857Z","iopub.execute_input":"2021-06-10T22:54:58.997168Z","iopub.status.idle":"2021-06-10T22:55:04.541951Z","shell.execute_reply.started":"2021-06-10T22:54:58.997139Z","shell.execute_reply":"2021-06-10T22:55:04.541075Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["train_df.head()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:55:04.543323Z","iopub.execute_input":"2021-06-10T22:55:04.543673Z","iopub.status.idle":"2021-06-10T22:55:04.565942Z","shell.execute_reply.started":"2021-06-10T22:55:04.543638Z","shell.execute_reply":"2021-06-10T22:55:04.564757Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                    qid                                      question_text  \\\n0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n\n   target                              cleaned_question_text  \n0       0  how quebec nationalists see province nation 1960s  \n1       0   do adopted dog would encourage people adopt shop  \n2       0  why velocity affect time does velocity affect ...  \n3       0   how otto von guericke used magdeburg hemispheres  \n4       0  can i convert montra helicon d mountain bike c...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>question_text</th>\n      <th>target</th>\n      <th>cleaned_question_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00002165364db923c7e6</td>\n      <td>How did Quebec nationalists see their province...</td>\n      <td>0</td>\n      <td>how quebec nationalists see province nation 1960s</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000032939017120e6e44</td>\n      <td>Do you have an adopted dog, how would you enco...</td>\n      <td>0</td>\n      <td>do adopted dog would encourage people adopt shop</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0000412ca6e4628ce2cf</td>\n      <td>Why does velocity affect time? Does velocity a...</td>\n      <td>0</td>\n      <td>why velocity affect time does velocity affect ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000042bf85aa498cd78e</td>\n      <td>How did Otto von Guericke used the Magdeburg h...</td>\n      <td>0</td>\n      <td>how otto von guericke used magdeburg hemispheres</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0000455dfa3e01eae3af</td>\n      <td>Can I convert montra helicon D to a mountain b...</td>\n      <td>0</td>\n      <td>can i convert montra helicon d mountain bike c...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n","\n","\n","punctuation = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', \n","    '•', '~', '@', '£', '·', '_', '{', '}', '©', '^', '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', \n","    '█', '…', '“', '★', '”', '–', '●', '►', '−', '¢', '¬', '░', '¡', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', \n","    '—', '‹', '─', '▒', '：', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', '¯', '♦', '¤', '▲', '¸', '⋅', '‘', '∞', \n","    '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '・', '╦', '╣', '╔', '╗', '▬', '❤', '≤', '‡', '√', '◄', '━', \n","    '⇒', '▶', '≥', '╝', '♡', '◊', '。', '✈', '≡', '☺', '✔', '↵', '≈', '✓', '♣', '☎', '℃', '◦', '└', '‟', '～', '！', '○', \n","    '◆', '№', '♠', '▌', '✿', '▸', '⁄', '□', '❖', '✦', '．', '÷', '｜', '┃', '／', '￥', '╠', '↩', '✭', '▐', '☼', '☻', '┐', \n","    '├', '«', '∼', '┌', '℉', '☮', '฿', '≦', '♬', '✧', '〉', '－', '⌂', '✖', '･', '◕', '※', '‖', '◀', '‰', '\\x97', '↺', \n","    '∆', '┘', '┬', '╬', '،', '⌘', '⊂', '＞', '〈', '⎙', '？', '☠', '⇐', '▫', '∗', '∈', '≠', '♀', '♔', '˚', '℗', '┗', '＊', \n","    '┼', '❀', '＆', '∩', '♂', '‿', '∑', '‣', '➜', '┛', '⇓', '☯', '⊖', '☀', '┳', '；', '∇', '⇑', '✰', '◇', '♯', '☞', '´', \n","    '↔', '┏', '｡', '◘', '∂', '✌', '♭', '┣', '┴', '┓', '✨', '\\xa0', '˜', '❥', '┫', '℠', '✒', '［', '∫', '\\x93', '≧', '］', \n","    '\\x94', '∀', '♛', '\\x96', '∨', '◎', '↻', '⇩', '＜', '≫', '✩', '✪', '♕', '؟', '₤', '☛', '╮', '␊', '＋', '┈', '％', \n","    '╋', '▽', '⇨', '┻', '⊗', '￡', '।', '▂', '✯', '▇', '＿', '➤', '✞', '＝', '▷', '△', '◙', '▅', '✝', '∧', '␉', '☭', \n","    '┊', '╯', '☾', '➔', '∴', '\\x92', '▃', '↳', '＾', '׳', '➢', '╭', '➡', '＠', '⊙', '☢', '˝', '∏', '„', '∥', '❝', '☐', \n","    '▆', '╱', '⋙', '๏', '☁', '⇔', '▔', '\\x91', '➚', '◡', '╰', '\\x85', '♢', '˙', '۞', '✘', '✮', '☑', '⋆', 'ⓘ', '❒', '☣', '✉', '⌊', '➠', '∣', '❑', '◢', 'ⓒ', '\\x80', '〒', '∕', '▮', '⦿', '✫', '✚', '⋯', '♩', '☂', '❞', '‗', '܂', '☜', \n","    '‾', '✜', '╲', '∘', '⟩', '＼', '⟨', '·', '✗', '♚', '∅', 'ⓔ', '◣', '͡', '‛', '❦', '◠', '✄', '❄', '∃', '␣', '≪', '｢', \n","    '≅', '◯', '☽', '∎', '｣', '❧', '̅', 'ⓐ', '↘', '⚓', '▣', '˘', '∪', '⇢', '✍', '⊥', '＃', '⎯', '↠', '۩', '☰', '◥', \n","    '⊆', '✽', '⚡', '↪', '❁', '☹', '◼', '☃', '◤', '❏', 'ⓢ', '⊱', '➝', '̣', '✡', '∠', '｀', '▴', '┤', '∝', '♏', 'ⓐ', \n","    '✎', ';', '␤', '＇', '❣', '✂', '✤', 'ⓞ', '☪', '✴', '⌒', '˛', '♒', '＄', '✶', '▻', 'ⓔ', '◌', '◈', '❚', '❂', '￦', \n","    '◉', '╜', '̃', '✱', '╖', '❉', 'ⓡ', '↗', 'ⓣ', '♻', '➽', '׀', '✲', '✬', '☉', '▉', '≒', '☥', '⌐', '♨', '✕', 'ⓝ', \n","    '⊰', '❘', '＂', '⇧', '̵', '➪', '▁', '▏', '⊃', 'ⓛ', '‚', '♰', '́', '✏', '⏑', '̶', 'ⓢ', '⩾', '￠', '❍', '≃', '⋰', '♋', \n","    '､', '̂', '❋', '✳', 'ⓤ', '╤', '▕', '⌣', '✸', '℮', '⁺', '▨', '╨', 'ⓥ', '♈', '❃', '☝', '✻', '⊇', '≻', '♘', '♞', \n","    '◂', '✟', '⌠', '✠', '☚', '✥', '❊', 'ⓒ', '⌈', '❅', 'ⓡ', '♧', 'ⓞ', '▭', '❱', 'ⓣ', '∟', '☕', '♺', '∵', '⍝', 'ⓑ', \n","    '✵', '✣', '٭', '♆', 'ⓘ', '∶', '⚜', '◞', '்', '✹', '➥', '↕', '̳', '∷', '✋', '➧', '∋', '̿', 'ͧ', '┅', '⥤', '⬆', '⋱', \n","    '☄', '↖', '⋮', '۔', '♌', 'ⓛ', '╕', '♓', '❯', '♍', '▋', '✺', '⭐', '✾', '♊', '➣', '▿', 'ⓑ', '♉', '⏠', '◾', '▹', \n","    '⩽', '↦', '╥', '⍵', '⌋', '։', '➨', '∮', '⇥', 'ⓗ', 'ⓓ', '⁻', '⎝', '⌥', '⌉', '◔', '◑', '✼', '♎', '♐', '╪', '⊚', \n","    '☒', '⇤', 'ⓜ', '⎠', '◐', '⚠', '╞', '◗', '⎕', 'ⓨ', '☟', 'ⓟ', '♟', '❈', '↬', 'ⓓ', '◻', '♮', '❙', '♤', '∉', '؛', \n","    '⁂', 'ⓝ', '־', '♑', '╫', '╓', '╳', '⬅', '☔', '☸', '┄', '╧', '׃', '⎢', '❆', '⋄', '⚫', '̏', '☏', '➞', '͂', '␙', 'ⓤ', '◟', '̊', '⚐', '✙', '↙', '̾', '℘', '✷', '⍺', '❌', '⊢', '▵', '✅', 'ⓖ', '☨', '▰', '╡', 'ⓜ', '☤', '∽', '╘', \n","    '˹', '↨', '♙', '⬇', '♱', '⌡', '⠀', '╛', '❕', '┉', 'ⓟ', '̀', '♖', 'ⓚ', '┆', '⎜', '◜', '⚾', '⤴', '✇', '╟', '⎛', \n","    '☩', '➲', '➟', 'ⓥ', 'ⓗ', '⏝', '◃', '╢', '↯', '✆', '˃', '⍴', '❇', '⚽', '╒', '̸', '♜', '☓', '➳', '⇄', '☬', '⚑', \n","    '✐', '⌃', '◅', '▢', '❐', '∊', '☈', '॥', '⎮', '▩', 'ு', '⊹', '‵', '␔', '☊', '➸', '̌', '☿', '⇉', '⊳', '╙', 'ⓦ', \n","    '⇣', '｛', '̄', '↝', '⎟', '▍', '❗', '״', '΄', '▞', '◁', '⛄', '⇝', '⎪', '♁', '⇠', '☇', '✊', 'ி', '｝', '⭕', '➘', \n","    '⁀', '☙', '❛', '❓', '⟲', '⇀', '≲', 'ⓕ', '⎥', '\\u06dd', 'ͤ', '₋', '̱', '̎', '♝', '≳', '▙', '➭', '܀', 'ⓖ', '⇛', '▊', \n","    '⇗', '̷', '⇱', '℅', 'ⓧ', '⚛', '̐', '̕', '⇌', '␀', '≌', 'ⓦ', '⊤', '̓', '☦', 'ⓕ', '▜', '➙', 'ⓨ', '⌨', '◮', '☷', \n","    '◍', 'ⓚ', '≔', '⏩', '⍳', '℞', '┋', '˻', '▚', '≺', 'ْ', '▟', '➻', '̪', '⏪', '̉', '⎞', '┇', '⍟', '⇪', '▎', '⇦', '␝', \n","    '⤷', '≖', '⟶', '♗', '̴', '♄', 'ͨ', '̈', '❜', '̡', '▛', '✁', '➩', 'ா', '˂', '↥', '⏎', '⎷', '̲', '➖', '↲', '⩵', '̗', '❢', \n","    '≎', '⚔', '⇇', '̑', '⊿', '̖', '☍', '➹', '⥊', '⁁', '✢']\n"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:40:17.067966Z","iopub.execute_input":"2021-06-10T13:40:17.068302Z","iopub.status.idle":"2021-06-10T13:40:17.110514Z","shell.execute_reply.started":"2021-06-10T13:40:17.068269Z","shell.execute_reply":"2021-06-10T13:40:17.109675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_text(txt, contraction_dict, punctuation):\n","    \"\"\"\"\"\n","    cleans the input text in the following steps\n","    1- replace contractions\n","    2- removing punctuation\n","    3- spliting into words\n","    4- removing stopwords\n","    \"\"\"\"\"\n","\n","    def _get_contraction(contraction_dict):\n","        contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n","        return contraction_dict, contraction_re\n","\n","    # replace contractions\n","    def remove_contraction(text, contraction_dict):\n","        contractions, contractions_re = _get_contraction(contraction_dict)\n","        def replace(match):\n","            return contractions[match.group(0)]\n","        return contractions_re.sub(replace, text)\n","    \n","    # remove punctuations\n","    def remove_punctuation(text):\n","        txt  = \"\".join([char for char in text if char not in punctuation])\n","        return re.sub(\"[^a-zA-Z0-9]+\", ' ', txt)\n","    \n","    # remove stopword\n","    def remove_stopword(words):\n","        stop_words = set(stopwords.words('english'))\n","        words = [w for w in words if not w in stop_words]\n","        return words\n","\n","    # to lower case\n","    def to_lower(words):\n","        return words.lower()\n","\n","    txt = remove_contraction(txt, contraction_dict)\n","    txt = remove_punctuation(txt)\n","    # split into words\n","    words = word_tokenize(txt)\n","    # words = remove_stopword(words)\n","    \n","    cleaned_text = ' '.join(words)\n","    \n","    # to lower case\n","    cleaned_text = to_lower(cleaned_text)\n","    return cleaned_text"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T13:59:55.213414Z","iopub.execute_input":"2021-06-10T13:59:55.213746Z","iopub.status.idle":"2021-06-10T13:59:55.224386Z","shell.execute_reply.started":"2021-06-10T13:59:55.213716Z","shell.execute_reply":"2021-06-10T13:59:55.223566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Start preprocessing train dataset and test dataset\n","tqdm.pandas()\n","train_df['cleaned_question_text'] = train_df['question_text'].progress_apply(lambda txt: clean_text(txt, contraction_dict, punctuation))\n","test_df['cleaned_question_text']  = test_df['question_text'].progress_apply(lambda txt: clean_text(txt, contraction_dict, punctuation))"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T14:01:24.251217Z","iopub.execute_input":"2021-06-10T14:01:24.25153Z","iopub.status.idle":"2021-06-10T14:23:23.021682Z","shell.execute_reply.started":"2021-06-10T14:01:24.251502Z","shell.execute_reply":"2021-06-10T14:23:23.020685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Notes**: Althought, this training (and testing) dataset are already chose very careful, I found that there are still some non English text inside the training and testing dataset. \n","\n","We can easily remove rows that have `non English text` in training dataset, but we *can not* remove it if they were in the test set."],"metadata":{}},{"cell_type":"code","source":["# plot and remove NaN value in train_set\n","nan_rows = train_df[train_df['cleaned_question_text'].isnull()]\n","nan_rows"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:55:11.492673Z","iopub.execute_input":"2021-06-10T22:55:11.492994Z","iopub.status.idle":"2021-06-10T22:55:11.605933Z","shell.execute_reply.started":"2021-06-10T22:55:11.492965Z","shell.execute_reply":"2021-06-10T22:55:11.605034Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [qid, question_text, target, cleaned_question_text]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>question_text</th>\n      <th>target</th>\n      <th>cleaned_question_text</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["train_df = train_df[train_df['cleaned_question_text'].notna()]"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T14:37:13.097112Z","iopub.execute_input":"2021-06-10T14:37:13.097444Z","iopub.status.idle":"2021-06-10T14:37:13.326551Z","shell.execute_reply.started":"2021-06-10T14:37:13.097415Z","shell.execute_reply":"2021-06-10T14:37:13.325656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:** when I was testing my code, I saved a version of preprocessed train_set to save some time"],"metadata":{}},{"cell_type":"code","source":["# compression_opts = dict(method='zip', archive_name='out.csv')  \n","# train_df.to_csv(\"./train_processed.zip\", index=False, compression=compression_opts)\n","\n","train_df.to_csv('./train_preprocessed.csv', index=False)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T14:37:17.088321Z","iopub.execute_input":"2021-06-10T14:37:17.088641Z","iopub.status.idle":"2021-06-10T14:37:25.894169Z","shell.execute_reply.started":"2021-06-10T14:37:17.088609Z","shell.execute_reply":"2021-06-10T14:37:25.893087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# II. Tokenizate dataset and embedding word\n","\n","Tokenization and embedding words are the process of transform text into a more digestible form so that machine learning algorithms can perform better."],"metadata":{}},{"cell_type":"markdown","source":["## Tokenizate dataset"],"metadata":{}},{"cell_type":"code","source":["TEXT  = data.Field(tokenize='spacy', batch_first=True, include_lengths=True)\n","LABEL = data.LabelField(dtype = torch.int64, batch_first=True)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:55:22.575608Z","iopub.execute_input":"2021-06-10T22:55:22.575932Z","iopub.status.idle":"2021-06-10T22:55:25.455405Z","shell.execute_reply.started":"2021-06-10T22:55:22.575900Z","shell.execute_reply":"2021-06-10T22:55:25.454485Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":["fields = [(None, None), (None,None), ('target', LABEL), ('text', TEXT)]\n","\n","# TabularDataset from torchtext only support to load from storage file\n","dataset = data.TabularDataset('../input/train-dataset-quora-insincere-questions/out.csv', format = 'csv', fields = fields, skip_header = True)"],"metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-10T22:55:36.604275Z","iopub.execute_input":"2021-06-10T22:55:36.604644Z","iopub.status.idle":"2021-06-10T22:57:51.399898Z","shell.execute_reply.started":"2021-06-10T22:55:36.604610Z","shell.execute_reply":"2021-06-10T22:57:51.398978Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":["# log data example\n","print(vars(dataset.examples[0]))"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:57:51.401229Z","iopub.execute_input":"2021-06-10T22:57:51.401591Z","iopub.status.idle":"2021-06-10T22:57:51.408095Z","shell.execute_reply.started":"2021-06-10T22:57:51.401555Z","shell.execute_reply":"2021-06-10T22:57:51.407195Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"{'target': '0', 'text': ['how', 'quebec', 'nationalists', 'see', 'province', 'nation', '1960s']}\n","output_type":"stream"}]},{"cell_type":"code","source":["import random\n","\n","SEED = 42\n","torch.manual_seed(SEED)\n","training_set, valid_set = dataset.split(split_ratio=0.8, random_state = random.seed(SEED))"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:57:51.410508Z","iopub.execute_input":"2021-06-10T22:57:51.411168Z","iopub.status.idle":"2021-06-10T22:57:53.464135Z","shell.execute_reply.started":"2021-06-10T22:57:51.411130Z","shell.execute_reply":"2021-06-10T22:57:53.463267Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Load GloVe word embedding"],"metadata":{}},{"cell_type":"code","source":["!unzip ../input/quora-insincere-questions-classification/embeddings.zip -d ./"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T14:42:05.532523Z","iopub.execute_input":"2021-06-10T14:42:05.53286Z","iopub.status.idle":"2021-06-10T14:45:43.358602Z","shell.execute_reply.started":"2021-06-10T14:42:05.53281Z","shell.execute_reply":"2021-06-10T14:45:43.357382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#due to the space limitation of kaggle, we must clear non use embedding.\n","NON_USE_DIR = ['./glove.840B.300d', './GoogleNews-vectors-negative300', './paragram_300_sl999']\n","\n","import os, shutil\n","def remove_dir(path):\n","    for filename in os.listdir(path):\n","        file_path = os.path.join(path, filename)\n","        try:\n","            if os.path.isfile(file_path) or os.path.islink(file_path):\n","                os.unlink(file_path)\n","            elif os.path.isdir(file_path):\n","                shutil.rmtree(file_path)\n","        except Exception as e:\n","            print('Failed to delete %s. Reason: %s' % (file_path, e))\n","    os.rmdir(path)\n","            \n","for dir in NON_USE_DIR:\n","    remove_dir(dir)\n","    "],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T14:45:43.3606Z","iopub.execute_input":"2021-06-10T14:45:43.361005Z","iopub.status.idle":"2021-06-10T14:45:48.059751Z","shell.execute_reply.started":"2021-06-10T14:45:43.360961Z","shell.execute_reply":"2021-06-10T14:45:48.058912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load embedding as storage file\n","import torchtext.vocab as vocab\n","\n","custom_embeddings = vocab.Vectors(name = '../input/glove6b/glove.6B.100d.txt')"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:57:53.465572Z","iopub.execute_input":"2021-06-10T22:57:53.465914Z","iopub.status.idle":"2021-06-10T22:58:27.143631Z","shell.execute_reply.started":"2021-06-10T22:57:53.465879Z","shell.execute_reply":"2021-06-10T22:58:27.142748Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"100%|█████████▉| 399999/400000 [00:27<00:00, 14422.92it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":["TEXT.build_vocab(training_set, min_freq=3, vectors = custom_embeddings)\n","LABEL.build_vocab(training_set)\n","\n","#No. of unique tokens in text\n","print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n","\n","#No. of unique tokens in label\n","print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n","\n","#Commonly used words\n","print(TEXT.vocab.freqs.most_common(10))  "],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:58:27.144881Z","iopub.execute_input":"2021-06-10T22:58:27.145393Z","iopub.status.idle":"2021-06-10T22:58:35.399182Z","shell.execute_reply.started":"2021-06-10T22:58:27.145346Z","shell.execute_reply":"2021-06-10T22:58:35.398249Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Size of TEXT vocabulary: 64853\nSize of LABEL vocabulary: 2\n[('what', 346962), ('i', 264607), ('how', 210414), ('why', 116259), ('is', 89048), ('would', 50611), ('get', 50099), ('best', 49881), ('people', 44434), ('can', 42469)]\n","output_type":"stream"}]},{"cell_type":"markdown","source":["# Model & Training\n","\n","In this challenge, we are building a basic bidirectional RNN to perform sentiment analysis task."],"metadata":{}},{"cell_type":"markdown","source":["## Bidirectional LSTM Model"],"metadata":{}},{"cell_type":"code","source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:58:55.910873Z","iopub.execute_input":"2021-06-10T22:58:55.911193Z","iopub.status.idle":"2021-06-10T22:58:55.916632Z","shell.execute_reply.started":"2021-06-10T22:58:55.911163Z","shell.execute_reply":"2021-06-10T22:58:55.914264Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class BidirectionalLSTM(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layer, output_size, dropout_rate=0.1):\n","        super(BidirectionalLSTM, self).__init__()\n","        self.dimension = hidden_size\n","        # Define layer\n","        self.embedding   = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm        = nn.LSTM(embedding_dim, hidden_size, num_layer, batch_first=True, bidirectional=True)\n","        self.dropout     = nn.Dropout(0.1)\n","        self.fc          = nn.Linear(hidden_size*2, output_size)\n","        self.sigmoid     = nn.Sigmoid()\n","\n","    def forward(self, input, input_len):\n","        # embedding word\n","        x = self.embedding(input)\n","        \n","        x = pack_padded_sequence(x, input_len.cpu(), batch_first=True, enforce_sorted=False)\n","        \n","        packed_output, (hidden, cell) = self.lstm(x)\n","        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n","        out_forward = output[range(len(output)), input_len.cpu() - 1, :self.dimension]\n","        out_reverse = output[:, 0, self.dimension:]\n","        out_reduced = torch.cat((out_forward, out_reverse), 1)\n","        text_feature = self.dropout(out_reduced)\n","\n","        text_feature = self.fc(text_feature).squeeze(1)\n","        return self.sigmoid(text_feature)\n","        \n","        \n","        # Notes!\n","#         output = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n","#         output = self.fc(output)\n","#         return self.sigmoid(output)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:59:06.780511Z","iopub.execute_input":"2021-06-10T22:59:06.780831Z","iopub.status.idle":"2021-06-10T22:59:06.790612Z","shell.execute_reply.started":"2021-06-10T22:59:06.780803Z","shell.execute_reply":"2021-06-10T22:59:06.789568Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class LSTM_GRU(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layer, output_size, dropout_rate=0.1):\n","        super(LSTM_GRU, self).__init__()\n","        # Define layer\n","        self.embedding   = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm        = nn.LSTM(embedding_dim, hidden_size, num_layer, batch_first=True, bidirectional=True)\n","        self.gru         = nn.GRU(hidden_size*2, hidden_size//2, num_layer, batch_first=True, bidirectional=True)\n","\n","        self.fc          = nn.Linear(hidden_size*3, output_size)\n","        \n","        self.emb_dropout = nn.Dropout2d(dropout_rate)\n","        self.sigmoid     = nn.Sigmoid()\n","\n","    def forward(self, input, input_len):\n","        # layer 1: embedding\n","        x = self.embedding(input)\n","        \n","        # layer 2: Spatial Dropout 1D\n","        embed = x.unsqueeze(2) # (N, T, 1, K)\n","        embed = embed.permute(0, 3, 2, 1)  # (N, K, 1, T)\n","        embed = self.emb_dropout(embed)  # (N, K, 1, T)\n","        embed = embed.permute(0, 3, 2, 1)  # (N, T, 1, K)\n","        x = embed.squeeze(2)  # (N, T, K)\n","        \n","        # layer 3: Bidirectional LSTM\n","        x = pack_padded_sequence(x, input_len.cpu(), batch_first=True, enforce_sorted=False) # (N, T, K)\n","        packed_lstm, h_lstm = self.lstm(x)\n","        \n","        # layer 4: Bidirectional GRU\n","        packed_gru, h_gru = self.gru(packed_lstm)\n","        \n","        packed_lstm = pad_packed_sequence(packed_lstm, batch_first=True)\n","        packed_gru = pad_packed_sequence(packed_gru, batch_first=True)\n","        \n","        # layer 5: Concat\n","        x = torch.cat((packed_gru[0], packed_lstm[0]), 2)\n","        \n","        # layer 6: Global Average Pool\n","        avg_pool = torch.mean(x, 1)\n","        \n","        # layer 7: Fully connected\n","        x = self.fc(avg_pool)\n","        x = self.sigmoid(x)\n","        return x"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:15:59.039538Z","iopub.execute_input":"2021-06-10T23:15:59.039864Z","iopub.status.idle":"2021-06-10T23:15:59.109513Z","shell.execute_reply.started":"2021-06-10T23:15:59.039834Z","shell.execute_reply":"2021-06-10T23:15:59.108626Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":["## Helper function "],"metadata":{}},{"cell_type":"code","source":["def binary_accuracy(preds, y):\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(preds)\n","    \n","    correct = (rounded_preds == y).float() \n","    acc = correct.sum() / len(correct)\n","    return acc\n","\n","def f1_loss(y_true:torch.Tensor, y_pred:torch.Tensor, is_training=False) -> torch.Tensor:\n","    '''Calculate F1 score. Can work with gpu tensors\n","    \n","    The original implmentation is written by Michal Haltuf on Kaggle.\n","    \n","    Returns\n","    -------\n","    torch.Tensor\n","        `ndim` == 1. 0 <= val <= 1\n","    \n","    Reference\n","    ---------\n","    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n","    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n","    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n","    \n","    '''\n","    assert y_true.ndim == 1\n","    assert y_pred.ndim == 1 or y_pred.ndim == 2\n","    \n","    if y_pred.ndim == 2:\n","        y_pred = y_pred.argmax(dim=1)\n","        \n","    \n","    tp = (y_true * y_pred).sum().to(torch.float32)\n","    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n","    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n","    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n","    \n","    epsilon = 1e-7\n","    \n","    precision = tp / (tp + fp + epsilon)\n","    recall = tp / (tp + fn + epsilon)\n","    \n","    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n","#     f1.requires_grad = is_training\n","    return f1"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:59:12.054926Z","iopub.execute_input":"2021-06-10T22:59:12.055241Z","iopub.status.idle":"2021-06-10T22:59:12.064224Z","shell.execute_reply.started":"2021-06-10T22:59:12.055212Z","shell.execute_reply":"2021-06-10T22:59:12.063230Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def train(model, device, train_iterator, optimizer, loss_function):\n","    model.train()\n","    running_loss = 0\n","    accuracy     = 0\n","    for i, (batch) in enumerate(train_iterator):\n","        # load data into cuda\n","        input, input_len = batch.text\n","        input, input_len = input.to(device), input_len.to(device)\n","\n","        # forward\n","        predict = model(input, input_len).squeeze()\n","        loss = loss_function(predict, batch.target.to(dtype=torch.float32, device=device))\n","\n","        # metric\n","        accuracy     += f1_loss(predict, batch.target)\n","        running_loss += loss.item()\n","        \n","        # zero the gradient + backprpagation + step\n","        optimizer.zero_grad()\n","\n","        loss.backward()\n","        optimizer.step()\n","        \n","    epoch_loss = running_loss/len(train_iterator)\n","    epoch_acc  = accuracy/len(train_iterator)\n","\n","    return epoch_loss, epoch_acc"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:47:29.833237Z","iopub.execute_input":"2021-06-10T23:47:29.833602Z","iopub.status.idle":"2021-06-10T23:47:29.840301Z","shell.execute_reply.started":"2021-06-10T23:47:29.833569Z","shell.execute_reply":"2021-06-10T23:47:29.839499Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["def test(model, device, test_iterator, loss_function):\n","    model.eval()\n","    running_loss = 0\n","    accuracy     = 0\n","    \n","    with torch.no_grad():\n","        for i, (batch) in enumerate(test_iterator):\n","            input, input_len = batch.text\n","            input, input_len = input.to(device), input_len.to(device)\n","\n","            predict = model(input, input_len).squeeze()\n","            loss = loss_function(predict, batch.target.to(dtype=torch.float32, device=device))\n","\n","            running_loss += loss.item()\n","            accuracy     += f1_loss(predict, batch.target)\n","\n","    epoch_loss = running_loss/len(test_iterator)\n","    epoch_acc  = accuracy/len(test_iterator)\n","    \n","    return epoch_loss, epoch_acc"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:47:30.328709Z","iopub.execute_input":"2021-06-10T23:47:30.329030Z","iopub.status.idle":"2021-06-10T23:47:30.335915Z","shell.execute_reply.started":"2021-06-10T23:47:30.328999Z","shell.execute_reply":"2021-06-10T23:47:30.334756Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["## Training process\n","\n","I used `Adam` from `torch` library as optimizer and `Binary Cross entropy` as loss function"],"metadata":{}},{"cell_type":"code","source":["# using cuda\n","device = f\"cuda:{torch.cuda.current_device()}\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Training on:\", torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"cpu\")\n","\n","# config\n","BATCH_SIZE       = 32\n","VOCAB_SIZE       = len(TEXT.vocab)\n","EMBEDDING_DIM    = 100\n","HIDDEN_SIZE      = 128\n","OUTPUT_SIZE      = 1\n","NUM_LAYER        = 2\n","\n","N_EPOCH          = 15"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:49:50.314655Z","iopub.execute_input":"2021-06-10T23:49:50.314963Z","iopub.status.idle":"2021-06-10T23:49:50.321108Z","shell.execute_reply.started":"2021-06-10T23:49:50.314934Z","shell.execute_reply":"2021-06-10T23:49:50.320016Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Training on: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":["#Load an iterator\n","train_iterator, valid_iterator = data.BucketIterator.splits(\n","                                    (training_set, valid_set), \n","                                    batch_size = BATCH_SIZE,\n","                                    sort_key = lambda x: len(x.text),\n","                                    sort_within_batch=True,\n","                                    device = device)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:47:35.225201Z","iopub.execute_input":"2021-06-10T23:47:35.225537Z","iopub.status.idle":"2021-06-10T23:47:35.230245Z","shell.execute_reply.started":"2021-06-10T23:47:35.225505Z","shell.execute_reply":"2021-06-10T23:47:35.229170Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["# init model\n","model = LSTM_GRU(vocab_size=VOCAB_SIZE, \n","             embedding_dim=EMBEDDING_DIM, \n","             hidden_size=HIDDEN_SIZE, \n","             num_layer=NUM_LAYER, \n","             output_size=OUTPUT_SIZE).to(device)\n","\n","model.load_state_dict(torch.load('./weights.pt'))\n","\n","# loss function & optimizer\n","# optimizer  = optim.Adam(model.parameters(), lr=0.0001)\n","optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.07)\n","\n","citeration = nn.BCELoss()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:49:52.581154Z","iopub.execute_input":"2021-06-10T23:49:52.581492Z","iopub.status.idle":"2021-06-10T23:49:52.680323Z","shell.execute_reply.started":"2021-06-10T23:49:52.581445Z","shell.execute_reply":"2021-06-10T23:49:52.679520Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["#Initialize the pretrained embedding\n","pretrained_embeddings = TEXT.vocab.vectors\n","model.embedding.weight.data.copy_(pretrained_embeddings)\n","\n","print(pretrained_embeddings.shape)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:00:03.995132Z","iopub.execute_input":"2021-06-10T23:00:03.995460Z","iopub.status.idle":"2021-06-10T23:00:04.007406Z","shell.execute_reply.started":"2021-06-10T23:00:03.995424Z","shell.execute_reply":"2021-06-10T23:00:04.006506Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"torch.Size([64853, 100])\n","output_type":"stream"}]},{"cell_type":"code","source":["# Start training\n","best_accuracy = 0\n","\n","for epoch in range(N_EPOCH):\n","    # train\n","    train_loss, train_accuracy = train(model, device, train_iterator, optimizer, citeration)\n","    \n","    # evaluate\n","    test_loss, test_accuracy = test(model, device, valid_iterator, citeration)\n","    \n","    #save the best model\n","    if test_accuracy > best_accuracy:\n","        best_accuracy = test_accuracy\n","        torch.save(model.state_dict(), './weights.pt')\n","    \n","    print(f'Epoch {epoch+1} summary ===========================')\n","    print(f'Train Loss: {train_loss:.3f} | Train F1 score: {train_accuracy*100:.2f}%')\n","    print(f' Val. Loss: {test_loss:.3f} |  Val. F1 score: {test_accuracy*100:.2f}%')"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:49:56.713856Z","iopub.execute_input":"2021-06-10T23:49:56.714164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualize training experiment"],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import plot_confusion_matrix"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epoch = range(0, len(train_losses))\n","\n","fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, sharex=True, figsize=(24, 6))\n","\n","ax0.plot(epoch, train_losses, 'g', label='Training loss')\n","ax0.plot(epoch, valid_losses, 'b', label='validation loss')\n","ax0.set_title('Training and Validation loss')\n","ax0.set_xlabel('Epochs')\n","ax0.set_ylabel('Loss')\n","ax0.legend()\n","\n","ax1.plot(epoch, train_res, 'g', label='Training F1 score')\n","ax1.plot(epoch, valid_res, 'b', label='validation F1 score')\n","ax1.set_title('Training and Validation F1 score')\n","ax1.set_xlabel('Epochs')\n","ax1.set_ylabel('F1 score')\n","ax1.legend()\n","\n","plt.show()"],"metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prediction and submission\n","\n","The submission file is `csv` format and contains 2 columns: \n","- `qid` is the ID of the question\n","- `preidction` is `1` or `0` refer as insincere of not.%load_ext tensorboard\n","%tensorboard --logdir logs%load_ext tensorboard\n","%tensorboard --logdir logs"],"metadata":{}},{"cell_type":"code","source":["# test dataset\n","test_df.head()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:26:17.786397Z","iopub.execute_input":"2021-06-10T18:26:17.786735Z","iopub.status.idle":"2021-06-10T18:26:17.802204Z","shell.execute_reply.started":"2021-06-10T18:26:17.7867Z","shell.execute_reply":"2021-06-10T18:26:17.801349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# helper function\n","import spacy\n","nlp = spacy.load('en')\n","\n","def predict(model, sentence):   \n","    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]  #tokenize the sentence # nlp.tokenizer(sentence)\n","    indexed = [TEXT.vocab.stoi[t] for t in tokenized]          #convert to integer sequence\n","    length = [len(indexed)]                                    #compute no. of words\n","    tensor = torch.LongTensor(indexed).to(device)              #convert to tensor\n","    tensor = tensor.unsqueeze(1).T                             #reshape in form of batch,no. of words\n","    length_tensor = torch.LongTensor(length)                   #convert to tensor\n","    try:\n","        prediction = model(tensor, length_tensor)              #prediction \n","    except:\n","        # print(\"Empty sentence:\", sentence)\n","        return 0\n","    \n","    return prediction.item()  "],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T18:26:23.107509Z","iopub.execute_input":"2021-06-10T18:26:23.107859Z","iopub.status.idle":"2021-06-10T18:26:24.536611Z","shell.execute_reply.started":"2021-06-10T18:26:23.107808Z","shell.execute_reply":"2021-06-10T18:26:24.535791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_prediction, prediction = [], []\n","THRES_HOLD = 0.32\n","\n","model.load_state_dict(torch.load('./weightsv3.pt'))\n","model.eval()\n","\n","for idx, row in test_df.iterrows():\n","    pred = 0\n","    raw_pred = predict(model, row['cleaned_question_text'])\n","    if raw_pred >= THRES_HOLD:\n","        pred = 1\n","        \n","    # save to list\n","    prediction.append(pred)\n","    raw_prediction.append(raw_pred)\n","\n","print('# of prediction', len(prediction))"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:19:32.431629Z","iopub.execute_input":"2021-06-10T21:19:32.431966Z","iopub.status.idle":"2021-06-10T21:41:54.122766Z","shell.execute_reply.started":"2021-06-10T21:19:32.431933Z","shell.execute_reply":"2021-06-10T21:41:54.121923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# merge\n","test_df['raw_prediction'] = raw_prediction\n","test_df['prediction'] = prediction\n","\n","test_df.head(10)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:41:54.124211Z","iopub.execute_input":"2021-06-10T21:41:54.124682Z","iopub.status.idle":"2021-06-10T21:41:54.371887Z","shell.execute_reply.started":"2021-06-10T21:41:54.124643Z","shell.execute_reply":"2021-06-10T21:41:54.371079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction_df = test_df.drop(['question_text', 'cleaned_question_text', 'raw_prediction'], axis=1)\n","prediction_df.to_csv('./submission.csv', index=False)\n","\n","prediction_df.head(10)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:41:54.373396Z","iopub.execute_input":"2021-06-10T21:41:54.373645Z","iopub.status.idle":"2021-06-10T21:41:55.171816Z","shell.execute_reply.started":"2021-06-10T21:41:54.373621Z","shell.execute_reply":"2021-06-10T21:41:55.170845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{},"execution_count":null,"outputs":[]}]}